{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Overview - Creating RAG System with OpenAI API and LangChain\n",
    "\n",
    "### This notebook demonstrates how to build a complete RAG (Retrieval Augmented Generation) system using OpenAI's GPT-4o model and embedding model with LangChain framework.\n",
    "\n",
    "## Main Features:\n",
    "### - Uses OpenAI GPT-4o as the generation model for answering questions\n",
    "### - Uses OpenAI text-embedding-3-small as the embedding model for document vectorization\n",
    "### - Supports multiple document formats (PDF, TXT, DOCX, etc.)\n",
    "### - Provides interactive Gradio web interface for user interaction\n",
    "### - Implements FAISS vector database for efficient similarity search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/miniconda3/envs/main/lib/python3.10/site-packages (1.84.0)\n",
      "Requirement already satisfied: langchain in /opt/miniconda3/envs/main/lib/python3.10/site-packages (0.3.25)\n",
      "Requirement already satisfied: langchain-openai in /opt/miniconda3/envs/main/lib/python3.10/site-packages (0.3.19)\n",
      "Requirement already satisfied: langchain-community in /opt/miniconda3/envs/main/lib/python3.10/site-packages (0.3.24)\n",
      "Requirement already satisfied: python-dotenv in /opt/miniconda3/envs/main/lib/python3.10/site-packages (1.1.0)\n",
      "Requirement already satisfied: gradio in /opt/miniconda3/envs/main/lib/python3.10/site-packages (5.31.0)\n",
      "Requirement already satisfied: faiss-cpu in /opt/miniconda3/envs/main/lib/python3.10/site-packages (1.11.0)\n",
      "Requirement already satisfied: pypdf in /opt/miniconda3/envs/main/lib/python3.10/site-packages (5.5.0)\n",
      "Requirement already satisfied: python-docx in /opt/miniconda3/envs/main/lib/python3.10/site-packages (1.1.2)\n",
      "Requirement already satisfied: unstructured in /opt/miniconda3/envs/main/lib/python3.10/site-packages (0.17.2)\n",
      "Requirement already satisfied: ipywidgets in /opt/miniconda3/envs/main/lib/python3.10/site-packages (8.1.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from openai) (2.11.5)\n",
      "Requirement already satisfied: sniffio in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from langchain) (0.3.63)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from langchain) (0.3.43)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from langchain-community) (3.12.4)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from langchain-community) (2.9.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from langchain-community) (2.2.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from gradio) (0.115.9)\n",
      "Requirement already satisfied: ffmpy in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.10.1 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from gradio) (1.10.1)\n",
      "Requirement already satisfied: groovy~=0.1 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from gradio) (0.32.2)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from gradio) (11.2.1)\n",
      "Requirement already satisfied: pydub in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from gradio) (0.11.12)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from gradio) (0.45.3)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from gradio) (0.16.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from gradio) (0.34.2)\n",
      "Requirement already satisfied: fsspec in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from gradio-client==1.10.1->gradio) (2025.3.0)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from python-docx) (5.4.0)\n",
      "Requirement already satisfied: chardet in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from unstructured) (5.2.0)\n",
      "Requirement already satisfied: filetype in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from unstructured) (1.2.0)\n",
      "Requirement already satisfied: python-magic in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from unstructured) (0.4.27)\n",
      "Requirement already satisfied: nltk in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from unstructured) (3.9.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from unstructured) (4.12.3)\n",
      "Requirement already satisfied: emoji in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from unstructured) (2.14.1)\n",
      "Requirement already satisfied: python-iso639 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from unstructured) (2025.2.18)\n",
      "Requirement already satisfied: langdetect in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from unstructured) (1.0.9)\n",
      "Requirement already satisfied: rapidfuzz in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from unstructured) (3.13.0)\n",
      "Requirement already satisfied: backoff in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from unstructured) (2.2.1)\n",
      "Requirement already satisfied: unstructured-client in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from unstructured) (0.36.0)\n",
      "Requirement already satisfied: wrapt in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from unstructured) (1.17.2)\n",
      "Requirement already satisfied: psutil in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from unstructured) (5.9.0)\n",
      "Requirement already satisfied: python-oxmsg in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from unstructured) (0.0.2)\n",
      "Requirement already satisfied: html5lib in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from unstructured) (1.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from ipywidgets) (8.30.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from huggingface-hub>=0.28.1->gradio) (1.1.3.dev0)\n",
      "Requirement already satisfied: decorator in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack-data in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: wcwidth in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from beautifulsoup4->unstructured) (2.5)\n",
      "Requirement already satisfied: webencodings in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from html5lib->unstructured) (0.5.1)\n",
      "Requirement already satisfied: joblib in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from nltk->unstructured) (1.5.1)\n",
      "Requirement already satisfied: olefile in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from python-oxmsg->unstructured) (0.47)\n",
      "Requirement already satisfied: executing in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: cryptography>=3.1 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from unstructured-client->unstructured) (45.0.3)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from unstructured-client->unstructured) (1.6.0)\n",
      "Requirement already satisfied: cffi>=1.14 in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /opt/miniconda3/envs/main/lib/python3.10/site-packages (from cffi>=1.14->cryptography>=3.1->unstructured-client->unstructured) (2.21)\n",
      "‚úÖ All dependencies installed successfully\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install Required Dependencies\n",
    "# This step installs all necessary Python packages for the RAG system including:\n",
    "# - openai: Official OpenAI Python client library\n",
    "# - langchain: Framework for building LLM applications\n",
    "# - langchain-openai: OpenAI integration for LangChain\n",
    "# - langchain-community: Community-contributed LangChain components\n",
    "# - python-dotenv: Environment variable management\n",
    "# - gradio: Web interface for ML applications\n",
    "# - faiss-cpu: Facebook AI Similarity Search library for vector operations\n",
    "# - pypdf, python-docx, unstructured: Document processing libraries\n",
    "# - ipywidgets: Interactive widgets for Jupyter notebooks\n",
    "\n",
    "!pip install openai langchain langchain-openai langchain-community python-dotenv gradio faiss-cpu pypdf python-docx unstructured ipywidgets\n",
    "\n",
    "print(\"‚úÖ All dependencies installed successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë API key configured (first 10 characters: sk-proj-j4...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë API key configured (first 10 characters: sk-proj-j4...)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Configure OpenAI API Key\n",
    "# This step handles the secure configuration of your OpenAI API key\n",
    "# The key can be loaded from:\n",
    "# 1. Environment variables (.env file)\n",
    "# 2. Direct user input (secure prompt)\n",
    "# The API key is required for accessing OpenAI's GPT-4o and embedding models\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import getpass\n",
    "\n",
    "# Load environment variables from .env file if it exists\n",
    "load_dotenv()\n",
    "\n",
    "# Attempt to get OpenAI API key from environment variables first\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# If not found in environment, prompt user to enter it securely\n",
    "if not openai_api_key:\n",
    "    openai_api_key = getpass.getpass(\"Please enter your OpenAI API key: \")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "# Verify that the API key has been successfully configured\n",
    "if openai_api_key:\n",
    "    print(f\"üîë API key configured (first 10 characters: {openai_api_key[:10]}...)\")\n",
    "else:\n",
    "    print(\"‚ùå API key not configured, please set your OpenAI API key\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚¨áÔ∏è Downloading: Accurate Timekeeping Supervisors 12.2.20_AH edits.docx ...\n",
      "‚úÖ Saved to: EllerDocs-FLSA/Accurate Timekeeping Supervisors 12.2.20_AH edits.docx\n",
      "‚¨áÔ∏è Downloading: Eller FLSA information 9.2024_AH edits.docx ...\n",
      "‚úÖ Saved to: EllerDocs-FLSA/Eller FLSA information 9.2024_AH edits.docx\n",
      "‚¨áÔ∏è Downloading: Eller Overtime Guidelines.docx ...\n",
      "‚úÖ Saved to: EllerDocs-FLSA/Eller Overtime Guidelines.docx\n",
      "‚¨áÔ∏è Downloading: Employee-Guide-Accurate-Timekeeping_AH edits.docx ...\n",
      "‚úÖ Saved to: EllerDocs-FLSA/Employee-Guide-Accurate-Timekeeping_AH edits.docx\n",
      "‚¨áÔ∏è Downloading: Salary-vs-Hourly-Guide_AH edits.docx ...\n",
      "‚úÖ Saved to: EllerDocs-FLSA/Salary-vs-Hourly-Guide_AH edits.docx\n",
      "‚¨áÔ∏è Downloading: Supervisors-Guide-Accurate-Timekeeping_AH edits.docx ...\n",
      "‚úÖ Saved to: EllerDocs-FLSA/Supervisors-Guide-Accurate-Timekeeping_AH edits.docx\n",
      "\n",
      "All downloads finished!\n",
      "Found 6 FLSA-related documents:\n",
      "   - Accurate Timekeeping Supervisors 12.2.20_AH edits.docx\n",
      "   - Salary-vs-Hourly-Guide_AH edits.docx\n",
      "   - Employee-Guide-Accurate-Timekeeping_AH edits.docx\n",
      "   - Eller Overtime Guidelines.docx\n",
      "   - Supervisors-Guide-Accurate-Timekeeping_AH edits.docx\n",
      "   - Eller FLSA information 9.2024_AH edits.docx\n",
      "‚úÖ FLSA documents prepared successfully\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Prepare Example Documents\n",
    "# This step identifies and prepares the documents that will be used for the RAG system\n",
    "# Priority: FLSA documents from EllerDocs-FLSA folder, fallback to default PDF\n",
    "# These documents will be processed, embedded, and stored in the vector database\n",
    "\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from urllib.parse import unquote  # Decode %20 and other escapes\n",
    "\n",
    "# Remote raw URLs of the DOCX files\n",
    "flsa_urls = [\n",
    "    \"https://github.com/JiakaiDu233/RAG_LangChain_OpenAI/raw/refs/heads/main/EllerDocs-FLSA/Accurate%20Timekeeping%20Supervisors%2012.2.20_AH%20edits.docx\",\n",
    "    \"https://github.com/JiakaiDu233/RAG_LangChain_OpenAI/raw/refs/heads/main/EllerDocs-FLSA/Eller%20FLSA%20information%209.2024_AH%20edits.docx\",\n",
    "    \"https://github.com/JiakaiDu233/RAG_LangChain_OpenAI/raw/refs/heads/main/EllerDocs-FLSA/Eller%20Overtime%20Guidelines.docx\",\n",
    "    \"https://github.com/JiakaiDu233/RAG_LangChain_OpenAI/raw/refs/heads/main/EllerDocs-FLSA/Employee-Guide-Accurate-Timekeeping_AH%20edits.docx\",\n",
    "    \"https://github.com/JiakaiDu233/RAG_LangChain_OpenAI/raw/refs/heads/main/EllerDocs-FLSA/Salary-vs-Hourly-Guide_AH%20edits.docx\",\n",
    "    \"https://github.com/JiakaiDu233/RAG_LangChain_OpenAI/raw/refs/heads/main/EllerDocs-FLSA/Supervisors-Guide-Accurate-Timekeeping_AH%20edits.docx\",\n",
    "]\n",
    "\n",
    "# Target folder: same directory as this script/notebook\n",
    "target_dir = Path(\"EllerDocs-FLSA\")\n",
    "target_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Download each file\n",
    "for url in flsa_urls:\n",
    "    file_name = unquote(url.split(\"/\")[-1])   # Restore spaces and other characters\n",
    "    file_path = target_dir / file_name\n",
    "\n",
    "    # Skip if file already exists (optional)\n",
    "    if file_path.exists():\n",
    "        print(f\"‚úîÔ∏è Already exists: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"‚¨áÔ∏è Downloading: {file_name} ...\")\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        file_path.write_bytes(resp.content)\n",
    "        print(f\"‚úÖ Saved to: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to download {file_name}: {e}\")\n",
    "\n",
    "print(\"\\nAll downloads finished!\")\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Check for FLSA documents in the EllerDocs-FLSA folder\n",
    "flsa_docs_folder = Path(\"EllerDocs-FLSA\")\n",
    "\n",
    "if flsa_docs_folder.exists():\n",
    "    # Discover all DOCX files in the FLSA documents folder\n",
    "    flsa_files = list(flsa_docs_folder.glob(\"*.docx\"))\n",
    "    print(f\"Found {len(flsa_files)} FLSA-related documents:\")\n",
    "    for file in flsa_files:\n",
    "        print(f\"   - {file.name}\")\n",
    "    \n",
    "    # Set the documents to be used (all files in the folder)\n",
    "    text_example_path = [str(file) for file in flsa_files]\n",
    "    print(\"‚úÖ FLSA documents prepared successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using FLSA-related documents (remote or pre-defined):\n",
      "   - EllerDocs-FLSA/Accurate Timekeeping Supervisors 12.2.20_AH edits.docx\n",
      "   - EllerDocs-FLSA/Salary-vs-Hourly-Guide_AH edits.docx\n",
      "   - EllerDocs-FLSA/Employee-Guide-Accurate-Timekeeping_AH edits.docx\n",
      "   - EllerDocs-FLSA/Eller Overtime Guidelines.docx\n",
      "   - EllerDocs-FLSA/Supervisors-Guide-Accurate-Timekeeping_AH edits.docx\n",
      "   - EllerDocs-FLSA/Eller FLSA information 9.2024_AH edits.docx\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Step 4 ‚Äì Validate and Set Document Sources\n",
    "# -------------------------------------------------------------\n",
    "# This step confirms which documents will be used by the RAG system\n",
    "# and falls back to local files if the preferred list is empty.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "if \"text_example_path\" in locals() and text_example_path:\n",
    "    print(\"Using FLSA-related documents (remote or pre-defined):\")\n",
    "    for doc in text_example_path:\n",
    "        print(f\"   - {doc}\")\n",
    "else:\n",
    "    # Fallback: search for local DOCX files under EllerDocs-FLSA/\n",
    "    local_dir = Path(\"EllerDocs-FLSA\")\n",
    "    local_docs = list(local_dir.glob(\"*.docx\")) if local_dir.exists() else []\n",
    "\n",
    "    if local_docs:\n",
    "        print(\"Using FLSA-related documents found locally:\")\n",
    "        for doc in local_docs:\n",
    "            print(f\"   - {doc.name}\")\n",
    "        text_example_path = [str(doc) for doc in local_docs]\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            \"No FLSA documents found in 'text_example_path' or in the local 'EllerDocs-FLSA' folder.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Initializing OpenAI Embedding model...\n",
      "Embedding model test successful\n",
      "Embedding dimension: 1536\n",
      "First 3 values: [-0.0023375607561320066, 0.05312768369913101, 0.03345499932765961]\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Initialize OpenAI Embedding Model\n",
    "# This step sets up the OpenAI text-embedding-3-small model which will be used to:\n",
    "# 1. Convert documents into vector representations (embeddings)\n",
    "# 2. Convert user queries into vector representations for similarity search\n",
    "# The embedding model is crucial for the retrieval component of RAG\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Initialize OpenAI embeddings with the text-embedding-3-small model\n",
    "print(\"üîÑ Initializing OpenAI Embedding model...\")\n",
    "embedding = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",  # OpenAI's efficient embedding model\n",
    "    openai_api_key=openai_api_key\n",
    ")\n",
    "\n",
    "# Test the embedding model to ensure it's working correctly\n",
    "test_text = \"This is a test document.\"\n",
    "embedding_result = embedding.embed_query(test_text)\n",
    "print(f\"Embedding model test successful\")\n",
    "print(f\"Embedding dimension: {len(embedding_result)}\")  # Should be 1536 for text-embedding-3-small\n",
    "print(f\"First 3 values: {embedding_result[:3]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Initializing OpenAI GPT-4o model...\n",
      "LLM model test successful\n",
      "Test response: 2 + 2 equals 4.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Initialize OpenAI GPT-4o Language Model\n",
    "# This step sets up the OpenAI GPT-4o model which will be used for:\n",
    "# 1. Generating answers based on retrieved document contexts\n",
    "# 2. Providing conversational AI capabilities\n",
    "# GPT-4o is OpenAI's most capable multimodal model with high reasoning ability\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize OpenAI LLM with GPT-4o model\n",
    "print(\"üîÑ Initializing OpenAI GPT-4o model...\")\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",           # OpenAI's most advanced model\n",
    "    temperature=0.1,          # Low temperature for more focused, deterministic responses\n",
    "    openai_api_key=openai_api_key\n",
    ")\n",
    "\n",
    "# Test the LLM to ensure it's working correctly\n",
    "test_question = \"What is 2 + 2?\"\n",
    "test_response = llm.invoke(test_question)\n",
    "print(f\"LLM model test successful\")\n",
    "print(f\"Test response: {test_response.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG components initialized\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Initialize RAG System Components\n",
    "# This step sets up all the essential components for the RAG system including:\n",
    "# 1. Text splitters for breaking documents into manageable chunks\n",
    "# 2. Document loaders for different file formats\n",
    "# 3. Vector store (FAISS) for similarity search\n",
    "# 4. Prompt template for structured question-answering\n",
    "# 5. Chain components for retrieval and generation\n",
    "\n",
    "import re\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    MarkdownTextSplitter,\n",
    ")\n",
    "from langchain.document_loaders import (\n",
    "    CSVLoader,\n",
    "    PyPDFLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    ")\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.docstore.document import Document\n",
    "import gradio as gr\n",
    "\n",
    "# Define available text splitters for different document processing strategies\n",
    "TEXT_SPLITERS = {\n",
    "    \"Character\": CharacterTextSplitter,              # Simple character-based splitting\n",
    "    \"RecursiveCharacter\": RecursiveCharacterTextSplitter,  # Smart recursive splitting (recommended)\n",
    "    \"Markdown\": MarkdownTextSplitter,                # Markdown-aware splitting\n",
    "}\n",
    "\n",
    "# Define document loaders for various file formats\n",
    "LOADERS = {\n",
    "    \".csv\": (CSVLoader, {}),\n",
    "    \".docx\": (UnstructuredWordDocumentLoader, {}),   # Microsoft Word documents\n",
    "    \".html\": (UnstructuredHTMLLoader, {}),           # HTML files\n",
    "    \".md\": (UnstructuredMarkdownLoader, {}),         # Markdown files\n",
    "    \".pdf\": (PyPDFLoader, {}),                       # PDF documents\n",
    "    \".pptx\": (UnstructuredPowerPointLoader, {}),     # PowerPoint presentations\n",
    "    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),      # Plain text files\n",
    "}\n",
    "\n",
    "# Define the RAG prompt template for structured question-answering\n",
    "rag_prompt_template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {input}\n",
    "Context: {context}\n",
    "Answer:\"\"\"\n",
    "\n",
    "print(\"RAG components initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Define Core RAG Functions\n",
    "# This step defines the essential functions for the RAG system including:\n",
    "# 1. Document loading and processing\n",
    "# 2. Vector database creation and management\n",
    "# 3. Retriever configuration and updates\n",
    "# 4. Chatbot functionality with RAG integration\n",
    "\n",
    "def load_single_document(file_path: str) -> list[Document]:\n",
    "    \"\"\"\n",
    "    Load a single document using the appropriate loader based on file extension\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the document file\n",
    "        \n",
    "    Returns:\n",
    "        List of Document objects containing the loaded content\n",
    "    \"\"\"\n",
    "    ext = \".\" + file_path.rsplit(\".\", 1)[-1]\n",
    "    if ext in LOADERS:\n",
    "        loader_class, loader_args = LOADERS[ext]\n",
    "        loader = loader_class(file_path, **loader_args)\n",
    "        return loader.load()\n",
    "    raise ValueError(f\"Unsupported file extension: '{ext}'\")\n",
    "\n",
    "def create_vectordb(\n",
    "    docs, spliter_name, chunk_size, chunk_overlap, vector_search_top_k, vector_rerank_top_n, run_rerank, search_method, score_threshold, progress=gr.Progress()\n",
    "):\n",
    "    \"\"\"\n",
    "    Initialize vector database with document processing and embedding creation\n",
    "    \n",
    "    Args:\n",
    "        docs: List of document paths to process\n",
    "        spliter_name: Text splitting strategy to use\n",
    "        chunk_size: Size of text chunks for processing\n",
    "        chunk_overlap: Overlap between consecutive chunks\n",
    "        vector_search_top_k: Number of top results to retrieve\n",
    "        vector_rerank_top_n: Number of results to rerank (not used in OpenAI version)\n",
    "        run_rerank: Whether to run reranking (not supported in OpenAI version)\n",
    "        search_method: Similarity search method to use\n",
    "        score_threshold: Minimum similarity score threshold\n",
    "        progress: Gradio progress tracker\n",
    "        \n",
    "    Returns:\n",
    "        Status message indicating success or failure\n",
    "    \"\"\"\n",
    "    global db\n",
    "    global retriever\n",
    "    global combine_docs_chain\n",
    "    global rag_chain\n",
    "    \n",
    "    import time\n",
    "\n",
    "    if vector_rerank_top_n > vector_search_top_k:\n",
    "        gr.Warning(\"Search top k must >= Rerank top n\")\n",
    "\n",
    "    print(f\"Starting document processing...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    documents = []\n",
    "    for doc in docs:\n",
    "        if type(doc) is not str:\n",
    "            doc = doc.name\n",
    "        documents.extend(load_single_document(doc))\n",
    "\n",
    "    text_splitter = TEXT_SPLITERS[spliter_name](chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    \n",
    "    print(f\"Document splitting completed, {len(texts)} chunks created\")\n",
    "    \n",
    "    # Create vector database - this will call Embedding API\n",
    "    print(f\"üîÑ Creating vector database (may take a few seconds)...\")\n",
    "    embed_start = time.time()\n",
    "    db = FAISS.from_documents(texts, embedding)\n",
    "    embed_time = time.time() - embed_start\n",
    "    print(f\"Vector database created, time taken: {embed_time:.2f} seconds\")\n",
    "    \n",
    "    if search_method == \"similarity_score_threshold\":\n",
    "        search_kwargs = {\"k\": vector_search_top_k, \"score_threshold\": score_threshold}\n",
    "    else:\n",
    "        search_kwargs = {\"k\": vector_search_top_k}\n",
    "    \n",
    "    retriever = db.as_retriever(search_kwargs=search_kwargs, search_type=search_method)\n",
    "    \n",
    "    # Note: rerank functionality is not enabled as OpenAI version doesn't have rerank model\n",
    "    if run_rerank:\n",
    "        print(\"‚ö†Ô∏è OpenAI version does not support rerank functionality\")\n",
    "    \n",
    "    prompt = PromptTemplate.from_template(rag_prompt_template)\n",
    "    combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    rag_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Vector database initialization completed, total time: {total_time:.2f} seconds\")\n",
    "    \n",
    "    return f\"Vector database is ready (time taken: {total_time:.1f} seconds)\"\n",
    "\n",
    "def update_retriever(vector_search_top_k, vector_rerank_top_n, run_rerank, search_method, score_threshold):\n",
    "    \"\"\"\n",
    "    Update retriever configuration with new parameters\n",
    "    \n",
    "    Args:\n",
    "        vector_search_top_k: Number of top results to retrieve\n",
    "        vector_rerank_top_n: Number of results to rerank (not used in OpenAI version)\n",
    "        run_rerank: Whether to run reranking (not supported in OpenAI version)\n",
    "        search_method: Similarity search method to use\n",
    "        score_threshold: Minimum similarity score threshold\n",
    "        \n",
    "    Returns:\n",
    "        Status message indicating successful update\n",
    "    \"\"\"\n",
    "    global retriever\n",
    "    global rag_chain\n",
    "\n",
    "    if vector_rerank_top_n > vector_search_top_k:\n",
    "        gr.Warning(\"Search top k must >= Rerank top n\")\n",
    "\n",
    "    if search_method == \"similarity_score_threshold\":\n",
    "        search_kwargs = {\"k\": vector_search_top_k, \"score_threshold\": score_threshold}\n",
    "    else:\n",
    "        search_kwargs = {\"k\": vector_search_top_k}\n",
    "    \n",
    "    retriever = db.as_retriever(search_kwargs=search_kwargs, search_type=search_method)\n",
    "    \n",
    "    if run_rerank:\n",
    "        print(\"‚ö†Ô∏è OpenAI version does not support rerank functionality\")\n",
    "    \n",
    "    rag_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "\n",
    "    return \"Retriever updated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Define Non-Streaming Bot Function\n",
    "# This step defines an alternative bot function that generates complete responses\n",
    "# at once instead of streaming. This version is simpler and more reliable for\n",
    "# certain use cases where streaming is not required.\n",
    "\n",
    "def bot(history,\n",
    "        temperature, top_p, top_k, repetition_penalty,\n",
    "        hide_full_prompt, do_rag):\n",
    "    \"\"\"\n",
    "    Non-streaming chatbot function with RAG integration\n",
    "    Generates complete answer and returns at once (more stable than streaming)\n",
    "    \n",
    "    Args:\n",
    "        history: Chat conversation history\n",
    "        temperature: LLM temperature for response randomness\n",
    "        top_p: LLM top-p sampling parameter\n",
    "        top_k: LLM top-k sampling parameter (not used in OpenAI)\n",
    "        repetition_penalty: Penalty for repetitive text\n",
    "        hide_full_prompt: Whether to hide retrieved context in response\n",
    "        do_rag: Whether to use RAG for response generation\n",
    "        \n",
    "    Yields:\n",
    "        Updated conversation history with complete response\n",
    "    \"\"\"\n",
    "    from langchain_core.messages import HumanMessage\n",
    "\n",
    "    # 0) First message\n",
    "    if not history:\n",
    "        yield history\n",
    "        return\n",
    "\n",
    "    # 1) OpenAI parameters\n",
    "    llm.temperature = temperature\n",
    "    llm.model_kwargs = {\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": repetition_penalty,\n",
    "    }\n",
    "\n",
    "    # 2) tuple ‚Üí list (for writing)\n",
    "    user_msg, _ = history[-1]\n",
    "    history[-1] = [user_msg, \"\"]          # Reserve assistant reply position\n",
    "\n",
    "    try:\n",
    "        # ---------- RAG ----------\n",
    "        if do_rag:\n",
    "            resp = rag_chain.invoke({\"input\": user_msg})\n",
    "            answer = resp.get(\"answer\", \"\")\n",
    "\n",
    "            # Whether to append retrieved content\n",
    "            if (not hide_full_prompt) and (\"context\" in resp):\n",
    "                ctx_docs = resp[\"context\"][:3]\n",
    "                if ctx_docs:\n",
    "                    ctx = \"\\n\\nüìÑ **Retrieved relevant content:**\\n\"\n",
    "                    for i, doc in enumerate(ctx_docs, 1):\n",
    "                        txt = getattr(doc, \"page_content\", str(doc))[:150]\n",
    "                        ctx += f\"{i}. {txt}...\\n\"\n",
    "                    answer += ctx\n",
    "\n",
    "        # ---------- Direct LLM ----------\n",
    "        else:\n",
    "            resp = llm.invoke([HumanMessage(content=user_msg)])\n",
    "            answer = resp.content if hasattr(resp, \"content\") else str(resp)\n",
    "\n",
    "        # Write final answer\n",
    "        history[-1][1] = answer\n",
    "        yield history[:-1] + [tuple(history[-1])]\n",
    "\n",
    "    except Exception as e:\n",
    "        history[-1][1] = f\"‚ùå Error occurred: {e}\"\n",
    "        yield history[:-1] + [tuple(history[-1])]\n",
    "\n",
    "    # Cleanup\n",
    "    history[-1] = tuple(history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Initializing vector database...\n",
      "Starting document processing...\n",
      "Document splitting completed, 129 chunks created\n",
      "üîÑ Creating vector database (may take a few seconds)...\n",
      "Vector database created, time taken: 5.04 seconds\n",
      "Vector database initialization completed, total time: 11.79 seconds\n",
      "Vector database is ready (time taken: 11.8 seconds)\n",
      "RAG system initialization successful!\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Initialize Vector Database with Example Documents\n",
    "# This step creates the vector database using the prepared documents\n",
    "# It processes documents, creates embeddings, and sets up the retrieval system\n",
    "# The vector database enables efficient similarity search for RAG\n",
    "\n",
    "# Initialize vector database using example documents with optimized settings\n",
    "print(\"üîÑ Initializing vector database...\")\n",
    "result = create_vectordb(\n",
    "    text_example_path,                 # Documents to process\n",
    "    \"RecursiveCharacter\",               # Use smart recursive text splitting\n",
    "    chunk_size=400,                     # Size of each text chunk\n",
    "    chunk_overlap=50,                   # Overlap between chunks for context\n",
    "    vector_search_top_k=4,              # Retrieve top 4 most similar chunks\n",
    "    vector_rerank_top_n=2,              # Rerank parameter (not used in OpenAI)\n",
    "    run_rerank=False,                   # Disable reranking for OpenAI version\n",
    "    search_method=\"similarity\",         # Use standard similarity search\n",
    "    score_threshold=0.5,                # Minimum similarity threshold\n",
    ")\n",
    "\n",
    "print(result)\n",
    "print(\"RAG system initialization successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Gradio interface...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiakaidu/RAG_openai/gradio_helper.py:176: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated gradio_helper.py file\n",
      "üîÑ Attempting to launch Gradio interface...\n",
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "‚ö†Ô∏è Local launch failed: Couldn't start the app because 'http://127.0.0.1:7861/gradio_api/startup-events' failed (code 502). Check your network or proxy settings to ensure localhost is accessible.\n",
      "üîÑ Attempting to launch with shared link...\n",
      "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
      "----\n",
      "* Running on public URL: https://62e5ca071afc1cd2d2.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://62e5ca071afc1cd2d2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting document processing...\n",
      "Document splitting completed, 129 chunks created\n",
      "üîÑ Creating vector database (may take a few seconds)...\n",
      "Vector database created, time taken: 5.59 seconds\n",
      "Vector database initialization completed, total time: 5.86 seconds\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7861 <> https://62e5ca071afc1cd2d2.gradio.live\n",
      "‚úÖ Shared link launch successful!\n",
      "\n",
      "üéâ RAG system has been launched!\n",
      "\n",
      "üìñ Usage instructions:\n",
      "1. Click 'Step 2: Build Vector Store' to build the vector database\n",
      "2. Enter questions in 'Step 3: Input Query'\n",
      "3. Upload new documents for Q&A\n",
      "4. Adjust RAG and LLM parameters in settings\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Launch Gradio Web Interface\n",
    "# This final step creates and launches the interactive web interface for the RAG system\n",
    "# The interface allows users to:\n",
    "# 1. Upload and process documents\n",
    "# 2. Build vector databases\n",
    "# 3. Ask questions and get RAG-powered answers\n",
    "# 4. Adjust system parameters in real-time\n",
    "\n",
    "# Launch interface using Gradio framework\n",
    "print(\"Starting Gradio interface...\")\n",
    "\n",
    "import requests\n",
    "\n",
    "# Ensure gradio_helper.py is available (download if not present)\n",
    "if not Path(\"gradio_helper.py\").exists():\n",
    "    r = requests.get(url=\"https://github.com/JiakaiDu233/RAG_LangChain_OpenAI/raw/refs/heads/main/gradio_helper.py\")\n",
    "    open(\"gradio_helper.py\", \"w\").write(r.text)\n",
    "    print(\"Updated gradio_helper.py file\")\n",
    "\n",
    "# Import the demo creation function\n",
    "from gradio_helper import make_demo\n",
    "\n",
    "# Create the Gradio demo interface with all RAG components\n",
    "demo = make_demo(\n",
    "    load_doc_fn=create_vectordb,         # Function to create vector database\n",
    "    run_fn=bot,                          # Chatbot function with RAG\n",
    "    stop_fn=None,                        # OpenAI version doesn't need stop functionality\n",
    "    update_retriever_fn=update_retriever, # Function to update retriever settings\n",
    "    model_name=\"GPT-4o\",                 # Display name for the model\n",
    "    language=\"English\",                  # Interface language\n",
    ")\n",
    "\n",
    "# Launch the interface with fallback to shared link if local fails\n",
    "try:\n",
    "    print(\"üîÑ Attempting to launch Gradio interface...\")\n",
    "    demo.queue().launch(debug=True)\n",
    "    print(\"‚úÖ Launch successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Local launch failed: {e}\")\n",
    "    print(\"üîÑ Attempting to launch with shared link...\")\n",
    "    demo.queue().launch(share=True, debug=True)\n",
    "    print(\"‚úÖ Shared link launch successful!\")\n",
    "\n",
    "print(\"\\nüéâ RAG system has been launched!\")\n",
    "print(\"\\nüìñ Usage instructions:\")\n",
    "print(\"1. Click 'Step 2: Build Vector Store' to build the vector database\")\n",
    "print(\"2. Enter questions in 'Step 3: Input Query'\")\n",
    "print(\"3. Upload new documents for Q&A\")\n",
    "print(\"4. Adjust RAG and LLM parameters in settings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (main)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
